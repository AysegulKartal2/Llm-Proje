{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# 1. Her ÅŸeyi temiz ve uyumlu kuran dev komut\n",
        "!pip install -q -U langchain langchain-community langchain-huggingface pypdf chromadb sentence-transformers accelerate bitsandbytes transformers\n",
        "\n",
        "# 2. Kurulum bittikten sonra otomatik durdurma (Hoca sorarsa 'ortamÄ± stabilize etmek iÃ§in yaptÄ±m' dersin)\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFKqHCVoNFHN",
        "outputId": "97f68f48-fb59-4a25-9d6a-ba8f347ec15f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m329.6/329.6 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.7/21.7 MB\u001b[0m \u001b[31m96.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m101.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m484.9/484.9 kB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m129.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m220.0/220.0 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m127.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "google-adk 1.21.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.39.1 which is incompatible.\n",
            "google-adk 1.21.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk<1.39.0,>=1.35.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "# 1. Drive'Ä± baÄŸla\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. KlasÃ¶r yolunu kontrol et\n",
        "# PDF'lerin tam olarak hangi klasÃ¶rdeyse orayÄ± yazmalÄ±sÄ±n.\n",
        "pdf_yolu = \"/content/drive/MyDrive/makaleler/\"\n",
        "\n",
        "documents = []\n",
        "if os.path.exists(pdf_yolu):\n",
        "    print(f\"ğŸ“‚ KlasÃ¶r bulundu: {pdf_yolu}\")\n",
        "    for file in os.listdir(pdf_yolu):\n",
        "        if file.endswith(\".pdf\"):\n",
        "            print(f\"ğŸ“„ Okunuyor: {file}\")\n",
        "            try:\n",
        "                loader = PyPDFLoader(os.path.join(pdf_yolu, file))\n",
        "                documents.extend(loader.load())\n",
        "            except Exception as e:\n",
        "                print(f\"âš ï¸ {file} okunurken hata: {e}\")\n",
        "\n",
        "    if len(documents) > 0:\n",
        "        print(f\"âœ… BaÅŸarÄ±lÄ±! Toplam {len(documents)} sayfa dÃ¶kÃ¼man yÃ¼klendi.\")\n",
        "    else:\n",
        "        print(\"âŒ HATA: KlasÃ¶rde PDF bulunamadÄ±. LÃ¼tfen PDF'leri doÄŸru klasÃ¶re attÄ±ÄŸÄ±ndan emin ol.\")\n",
        "else:\n",
        "    print(f\"âŒ HATA: '{pdf_yolu}' yolu bulunamadÄ±! Drive'daki klasÃ¶r ismini kontrol et.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxQtns2LP_xT",
        "outputId": "e05e0170-88ed-4c8d-c511-d99f230ac33e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "ğŸ“‚ KlasÃ¶r bulundu: /content/drive/MyDrive/makaleler/\n",
            "ğŸ“„ Okunuyor: NIPS-2017-attention-is-all-you-need-Paper.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pypdf._reader:Ignoring wrong pointing object 31 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 32 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 89 0 (offset 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“„ Okunuyor: NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf\n",
            "ğŸ“„ Okunuyor: ReAct Synergizing Reasoning and Acting in Language Models.pdf\n",
            "ğŸ“„ Okunuyor: The Llama 3 Herd of Models.pdf\n",
            "ğŸ“„ Okunuyor: Language Models are Few-Shot Learners (GPT-3 Makalesi).pdf\n",
            "ğŸ“„ Okunuyor: LLMWEEK1.pdf\n",
            "ğŸ“„ Okunuyor: LLMWEEK4.pdf\n",
            "ğŸ“„ Okunuyor: LLMWEEK2.pdf\n",
            "ğŸ“„ Okunuyor: LLMWEEK3.pdf\n",
            "âœ… BaÅŸarÄ±lÄ±! Toplam 305 sayfa dÃ¶kÃ¼man yÃ¼klendi.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "# 1. Metinleri Anlamsal ParÃ§alara BÃ¶lme (RAG MantÄ±ÄŸÄ±)\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=80)\n",
        "chunks = text_splitter.split_documents(documents)\n",
        "\n",
        "# 2. VektÃ¶r VeritabanÄ± OluÅŸturma (SBERT ile)\n",
        "print(\"ğŸ§  Anlamsal hafÄ±za oluÅŸturuluyor...\")\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "vector_db = Chroma.from_documents(chunks, embeddings)\n",
        "\n",
        "# 3. Llama-3 Modelini GPU'ya YÃ¼kleme (4-bit Kuantize)\n",
        "model_id = \"unsloth/llama-3-8b-Instruct-bnb-4bit\"\n",
        "print(\"ğŸ¤– Llama-3 GPU'ya yÃ¼kleniyor...\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "# 4. Pipeline AyarlarÄ± (TekrarÄ± engelleyen parametreler)\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=256,\n",
        "    temperature=0.2,\n",
        "    repetition_penalty=1.2,\n",
        "    do_sample=True\n",
        ")\n",
        "llm = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "# 5. Akademik Asistan Zinciri (Prompt Engineering)\n",
        "template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "Sen bir Akademik AsistansÄ±n.\n",
        "GÃ–REVÄ°N: Sana verilen baÄŸlamdaki bilgileri analiz etmek ve KESÄ°NLÄ°KLE TÃœRKÃ‡E cevap vermektir.\n",
        "KURAL 1: BaÄŸlam Ä°ngilizce olsa dahi cevabÄ±n %100 TÃ¼rkÃ§e olmalÄ±dÄ±r.\n",
        "KURAL 2: Teknik terimlerin TÃ¼rkÃ§e karÅŸÄ±lÄ±ÄŸÄ±nÄ± kullan veya parantez iÃ§inde belirt.\n",
        "KURAL 3: Asla Ä°ngilizce cÃ¼mle kurma.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "BAÄLAM: {context}\n",
        "SORU: {question}<|end_header_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": vector_db.as_retriever(search_kwargs={\"k\": 3}) | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(\"\\nğŸš€ SÄ°STEM HAZIR! ArtÄ±k akademik makalelerin hakkÄ±nda soru sorabilirsin.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KW0gDsjSQmqc",
        "outputId": "7ab88b6f-5d76-4c14-f17e-e48c2ea161f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ§  Anlamsal hafÄ±za oluÅŸturuluyor...\n",
            "ğŸ¤– Llama-3 GPU'ya yÃ¼kleniyor...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸš€ SÄ°STEM HAZIR! ArtÄ±k akademik makalelerin hakkÄ±nda soru sorabilirsin.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Sorusu: Makalelerindeki en temel konulardan birini soralÄ±m\n",
        "soru = \"Transformer mimarisindeki self-attention (Ã¶z-dikkat) mekanizmasÄ± nedir ve geleneksel RNN modellerinden farkÄ± nedir?\"\n",
        "\n",
        "print(\"ğŸ” DÃ¶kÃ¼manlar taranÄ±yor ve cevap Ã¼retiliyor...\")\n",
        "cevap = rag_chain.invoke(soru)\n",
        "\n",
        "# Llama-3'Ã¼n Ã¼rettiÄŸi ham metinden sadece asistanÄ±n cevabÄ±nÄ± ayÄ±klÄ±yoruz\n",
        "temiz_cevap = cevap.split(\"assistant\")[-1].strip()\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(f\"ğŸ¤” SORU: {soru}\")\n",
        "print(\"=\"*50)\n",
        "print(f\"ğŸ¤– AKADEMÄ°K ASÄ°STANIN CEVABI:\\n\\n{temiz_cevap}\")\n",
        "print(\"=\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lW-lfDYORhJl",
        "outputId": "13463d30-0849-48d3-c930-7d21e2665ccc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ” DÃ¶kÃ¼manlar taranÄ±yor ve cevap Ã¼retiliyor...\n",
            "\n",
            "==================================================\n",
            "ğŸ¤” SORU: Transformer mimarisindeki self-attention (Ã¶z-dikkat) mekanizmasÄ± nedir ve geleneksel RNN modellerinden farkÄ± nedir?\n",
            "==================================================\n",
            "ğŸ¤– AKADEMÄ°K ASÄ°STANIN CEVABI:\n",
            "\n",
            "<|end_header_id|>\n",
            "\n",
            "Transformer mimarisinde kullanÄ±lan Ã¶z-dikkat mekanizmasÄ±, bir dizi elemanlarÄ±nÄ±n birbirleriyle olan iliÅŸkiyi hesaplamak iÃ§in kullanÄ±lÄ±r. Bu mekanizmanÄ±n amacÄ±, dizi iÃ§indeki belli baÅŸlÄ± elemanlarÄ±n Ã¶nemini belirlemek ve bu elemanlara daha fazla dikkat etmektir.\n",
            "\n",
            "Bu mekanizmaya gÃ¶re, bir modele verilmiÅŸ bir metnin farklÄ± parÃ§alarÄ±na \"dikkat\" edilir. Herhangi bir parÃ§asÄ± diÄŸerlerine gÃ¶re daha Ã¶nemlidir, o zaman model ona daha fazla dikkat eder. BÃ¶ylece, model, Ã¶nemli olan parÃ§alarÄ± daha iyi anlayarak, metni daha etkili bir ÅŸekilde iÅŸleyebilir.\n",
            "\n",
            "Geleneksel RNN modellerine kÄ±yasla, Ã¶z-dikkat mekanizmasÄ±nÄ±n en bÃ¼yÃ¼k farkÄ±, her tarihte tÃ¼m Ã¶nceki adÄ±mlarÄ± hesaba katmak zorunda olmamasÄ±dÄ±r. Ã–z-dikkat mekanizmasÄ±, sadece ilgili parÃ§alara odaklanarak, hem hÄ±zlÄ± hem de yÃ¼ksek doÄŸrulukta sonuÃ§lar verir.\n",
            "\n",
            "Ã–rnek olarak, bir metinin baÅŸÄ±nda bulunan kelimenin, sonunda bulunan kelime ile ilgisi olabilir. Geleneksel RNN modelleri, bÃ¼tÃ¼n adÄ±mlarda bu iliÅŸkinin hesabÄ±nÄ± yaparken, Ã¶z-dikkat mekanizmasÄ± ise sadece bu iki kelimenin arasÄ±nda olan iliÅŸkinin hesabÄ±nÄ± yapar. Bu sayede, model, daha kÄ±sa sÃ¼rede ve daha doÄŸru sonuÃ§lar alÄ±r.\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gradio\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "def akademik_asistan_yanitla(soru):\n",
        "    \"\"\"KullanÄ±cÄ±dan gelen soruyu RAG zincirine gÃ¶nderir ve temiz TÃ¼rkÃ§e yanÄ±t dÃ¶ner.\"\"\"\n",
        "    if not soru.strip():\n",
        "        return \"LÃ¼tfen makaleler veya ders notlarÄ± hakkÄ±nda bir soru girin.\"\n",
        "\n",
        "    try:\n",
        "        # RAG zincirini Ã§alÄ±ÅŸtÄ±r\n",
        "        sonuc = rag_chain.invoke(soru)\n",
        "\n",
        "        # 1. Llama-3 formatÄ±ndaki 'assistant' etiketinden sonrasÄ±nÄ± al\n",
        "        if \"assistant\" in sonuc:\n",
        "            cevap = sonuc.split(\"assistant\")[-1]\n",
        "        else:\n",
        "            cevap = sonuc\n",
        "\n",
        "        # 2. Teknik kalÄ±ntÄ±larÄ± (<|end_header_id|> vb.) temizle\n",
        "        temiz_cevap = cevap.replace(\"<|end_header_id|>\", \"\").replace(\"<|eot_id|>\", \"\").strip()\n",
        "\n",
        "        return temiz_cevap\n",
        "    except Exception as e:\n",
        "        return f\"Sistemde bir hata oluÅŸtu: {str(e)}\"\n",
        "\n",
        "# ArayÃ¼zÃ¼n GÃ¶rsel TasarÄ±mÄ±\n",
        "with gr.Blocks(theme=gr.themes.Soft(primary_hue=\"blue\", secondary_hue=\"slate\")) as demo:\n",
        "    gr.Markdown(\"\"\"\n",
        "    # ğŸš€ LLM Akademik AraÅŸtÄ±rma AsistanÄ±\n",
        "    ### Dr. Ã–ÄŸr. Ãœyesi Murat ÅimÅŸek - 2024-2025 GÃ¼z DÃ¶nemi Final Projesi\n",
        "    **Veri KaynaÄŸÄ±:** 5 Akademik Makale (Transformer, RAG, GPT-3, Llama-3, ReAct) + 9 Ders SlaytÄ±\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=2):\n",
        "            soru_kutusu = gr.Textbox(\n",
        "                label=\"Sorunuzu Buraya YazÄ±n\",\n",
        "                placeholder=\"Ã–rn: Transformer mimarisinde 'Encoder' ve 'Decoder' farkÄ± nedir?\",\n",
        "                lines=4\n",
        "            )\n",
        "            gonder_btn = gr.Button(\"Analiz Et ve YanÄ±tla\", variant=\"primary\")\n",
        "            gr.Examples(\n",
        "                examples=[\"Self-attention mekanizmasÄ± nedir?\", \"Kuantizasyonun avantajlarÄ± nelerdir?\", \"RAG mimarisi nasÄ±l Ã§alÄ±ÅŸÄ±r?\"],\n",
        "                inputs=soru_kutusu\n",
        "            )\n",
        "\n",
        "        with gr.Column(scale=3):\n",
        "            cevap_kutusu = gr.Textbox(\n",
        "                label=\"Akademik AsistanÄ±n TÃ¼rkÃ§e YanÄ±tÄ±\",\n",
        "                lines=12,\n",
        "                interactive=False\n",
        "            )\n",
        "\n",
        "    gr.Markdown(\"---\")\n",
        "    gr.Markdown(\"â„¹ï¸ *Bu proje Llama-3-8B modeli Ã¼zerinde 4-bit kuantizasyon ve LangChain RAG mimarisi kullanÄ±larak geliÅŸtirilmiÅŸtir.*\")\n",
        "\n",
        "    # Buton tetikleyici\n",
        "    gonder_btn.click(fn=akademik_asistan_yanitla, inputs=soru_kutusu, outputs=cevap_kutusu)\n",
        "\n",
        "# Public URL oluÅŸturur\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "id": "weJ5s10xXp7u",
        "outputId": "7b9adbaf-d5c7-4fa9-8225-6bdfe3601b0d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3272346261.py:28: DeprecationWarning: The 'theme' parameter in the Blocks constructor will be removed in Gradio 6.0. You will need to pass 'theme' to Blocks.launch() instead.\n",
            "  with gr.Blocks(theme=gr.themes.Soft(primary_hue=\"blue\", secondary_hue=\"slate\")) as demo:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://8f25e7e6fb445bcc28.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://8f25e7e6fb445bcc28.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    }
  ]
}